### 1. Tiny URL 

> **Functional requirement: **
>
> > - Given a URL, service will generate a unique and shorter url
> > - When user click on the shorter url, our service will return original URL and redirect to the original link
> > - Tiny URL will be expired within a time period
> > - User may be able to customized a short URL
> > - Deleter URL
>
> **Non-Function requeirment:**
>
> > - Availiblity: The system should be high availability, otherwise Tiny URL redirections will be failed
> > - Scalability: System can be horizontally scalable by following CAP theorem 
> > - Latency: Redirection should happen in real-tim with mimium latency 
> > - Tiny URL should not be predicatable 
> > - Read heavy service
>
> **Extended requirement:**
>
> > The service should be accessed through RESTful API. 
> >
> > - Based on senario of when, where, how it happens
> > - Based on how easy to implement the service
>
> **Capacity Estimation and Constraints**
>
> > how many users we have? 
> > Let’s assuem we have 200M users.  let’s suppose each user will  generate 0.1 URL per day， redirected by 10 URLs/day
> >
> > - Write QPS
> >   - Average: 200,000,000* 0.1 / (24*3600) = 231.4 QPS , Let’s say 300 QPS
> >   - Peak: 300*2 = 600 QPS 
> > - Read QPS (100 times of write)
> >   - Average: 30K QPS
> >   - Peak: 60K QPS
> > - Storage(WRITE)
> >   - 500 byte for every url, 200,000,000* 0.1* 0.5 KB= 10,000,000 KB = 10000 MB = 10 GB / DAY
> >   - 10 TB distk can use 3 years, if we store 5 years, we need 20TB including backup
> > - Memory(READ)
> >   - Let’s follow 8/2 rule, means 20% of URLs taking 80% traffic, and we can cache 20% URLs per day
> >   - Every day, we need 200M * 0.5 KB* 10 *0.2 = 200,000,000 KB = 200GB /DAY
>
> **Service:**
>
> > - Functions
> >
> >   - Create Tiny URL
> >     `createURL(APIKey, userId, originalURL, customize_alias = None, expire_data = 5 years)`
> >
> >     API key can prevent abuse, and 
> >
> >   - Return original URL
> >
> >     `redirectURL(userId, tinyURL)`
> >
> >   - Delete Tiny URL
> >
> >     `deleteURL(userId, tinyURL)`
> >
> > - RESTful API
> >
> >   - GET /<tiny_url>
> >   - POST /tiny_url
> >     - Payload: {……) 
> >     - Return: short url
> >
> > **How do we detect and prevent abuse?** 
> > For instance, any service can put us out of business by consuming all our keys in the current design. To prevent abuse, we can limit users through their APIKey, how many URL they can create or access in a certain time.
>
> **Database:**
>
> > - SQL VS NonSQL
> >
> >   >  Since we are likely going to store billions of rows and we don’t need to use relationships between objects – a NoSQL key-value store like Dynamo or Cassandra is a better choice, which would also be easier to scale.
> >
> > - Schema
> >
> >   - user
> >
> >     > user_id: int PK
> >     >
> >     > user_name: varcher(50)
> >     >
> >     > email: varcher(50)
> >
> >   - url
> >
> >     > tiny_url: varchar(20) PK
> >     >
> >     > original_url: varcher(500) 
> >     >
> >     > expire_data: datatime 
> >
> >   - map(no join in nosql server)
> >
> >     > tiny_url: varchar(20) PK
> >     >
> >     > user_id: int 
>
> **Component Detail Design:**
>
> > 1. Encoding original url to tiny url
> >
> > > - **Option1**: user MD5, and only pick last of first 6 charters
> > >
> > >   - Pro: fast
> > >   - con: hash collisions, can not guartee unique
> > >
> > > - **Option2**: Sequential id  + base36 ([a-z ,0-9]) or base62 ([A-Z, a-z, 0-9]), if we add ‘-’ and ‘.’, we can use base64 encoding
> > >
> > >   > **Question1:** 
> > >   >
> > >   > We have the following couple of problems with our encoding scheme:
> > >   >
> > >   > 1. If multiple users enter the same URL, they can get the same shortened URL, which is not acceptable?
> > >   >
> > >   > 2. What if parts of the URL are URL-encoded? e.g., http://distributed.php?id=design, and http://distributed.php%3Fid%3Ddesign are identical except for the URL encoding
> > >   >
> > >   > **Solution:** 
> > >   >
> > >   > 1. use the Sql DB  auto incretement id feature
> > >   > 2. use the userId append to the input URLs, if user not sign in , might be a problem.
> > >
> > > - **Option3: **Generating keys offline(KGS).  
> > >   We can have a standalone Key Generation Service (KGS) that generates random six letter strings beforehand and stores them in a database (let’s call it key-DB). Whenever we want to shorten a URL, we will just take one of the already generated keys and use it. This approach will make things quite simple and fast since we will not be encoding the URL or worrying about duplications or collisions.
> > >
> > >   > **Question1:**
> > >   >
> > >   > Can concurrency cause problems?
> > >   >
> > >   > **Solution:**
> > >   >
> > >   > 1. Two tables, one for not used key, another for used key. If key is ued, moved it; Or, use flag to show key is used or not
> > >   > 2. Cache some unused key in memory, and mark them as used
> > >   > 3. Distributed lock if we have multiple web server
> > >
> > >   > **Question2:****
> > >   >
> > >   > Isn’t KGS the single point of failure?
> > >   >
> > >   > **Solution:**
> > >   >
> > >   > we can have a standby replica of KGS, and whenever the primary server dies, it can take over to generate and provide keys.
> > >
> > >   > **Question3:**
> > >   >
> > >   > user redirection use 301 or 302?
> > >   >
> > >   > **Solution**
> > >   >
> > >   > use 302. 用户第一次访问某个短链接后，如果服务器返回301状态码，则这个用户在后续多次访问统一短链接，浏览器会直接请求跳转地址，而不是短链接地址，这样一来服务器端就无法收到用户的请求。
> > >   >
> > >   > 如果服务器返回302状态码，且告知浏览器不缓存短链接请求，那么用户每次访问短链接，都会先去短链接服务端取回长链接地址，然后在跳转。
> > >   >
> > >   > 1. 从语义上来说，301跳转更为合适，因为是永久跳转，不会每次都访问服务端，还可以减小服务端压力。
> > >   > 2. 但如果使用301跳转，服务端就无法精确搜集用户的访问行为了。
> > >   > 3. 相反302跳转会导致服务端压力增大，但服务端此时就可精确搜集用户的访问行为。基于用户的访问行为，可以做一些分析，得出一些有意思的结论。比如可以根据用户IP地址得出用户区域分布情况，根据User-Agent消息头分析出用户使用不同的操作系统以及浏览器比例等等。
>
> **Scalability**
>
> > 1. **DB**
> >
> >    - Sharing
> >      - **Range Based Partitioning: **We can store **tiny URLs** in separate partitions based on the first letter of the URL or the hash key. Hence we save all the URLs starting with letter ‘A’ in one partition and those that start with letter ‘B’ into another partition and so on. This approach is called range based partitioning.   **May cause some overload partitions **, need to Monitor it or use consistanc hashing.
> >      - **Hash Function: **In our case, we can take the hash of the ‘key’ or the actual URL to determine the partition to store the file. **May cause some overload partitions **.
> >
> >    - **Replica**
> >
> >      > KGS can have read/write spliting, once write server is done, use read server as master
> >      >
> >    
> > 2. **Cache**
> >
> >    > - **Where we use**:
> >    >
> >    >   >  Use cache between app server and db, app server and kgs.
> >    >
> >    > - **How much we need**
> >    >
> >    >   > As estimated above we need 200GB memory to cache 20% of daily traffic since a modern day server can have 256GB memory, we can easily fit all the cache into one machine, or we can choose to use a couple of smaller servers to store all these hot URLs.
> >    >
> >    > - **Cache policy**
> >    >
> >    >   > Read-through
> >    >   > Write-through
> >    >
> >    > - **Cache eviction policy**
> >    >
> >    >   > Least Recently Used (LRU) can be a reasonable policy for our system. 
> >    >   
> >    > - **Cache replica**
> >    >
> >    >   > When we can not find url, we will go to the db. database. Whenever this happens, we can update the cache and pass the new entry to all the cache replicas. Each replica can update their cache by adding the new entry. If a replica already has that entry, it can simply ignore it.
> >    >
> >
> > 3. **Load Balancer (LB)**
> >
> >    > - **Where we use**:
> >    >
> >    >   > 1. Between Clients and Application servers
> >    >   > 2. Between Application Servers and database servers
> >    >   > 3. Between Application Servers and Cache servers
> >    >
> >    > - **Distribution strategy**
> >    >
> >    >   > 1. Round-robin
> >    >   > 2. **least_conn**, use this one to avoid case that If a server is overloaded or slow
> >
>

### 2. Vending Machine ?

> design software for vending machines sold Amazon Kindles
>
> **Functional requirement: **
>
> > - 用户可以通过自动售货机购买Amazon Kindle设备。
> > - 用户可以选择不同型号的Kindle设备。
> > - 用户可以使用多种支付方式（如信用卡、移动支付等）进行支付。
> > - 系统需要验证支付并在支付成功后提供设备。
> > - 系统需要显示库存信息，并在设备售罄时提示用户。
> > - 自动售货机需要提供购买凭证或电子收据。
> > - 管理员可以远程监控和管理售货机，包括库存管理和维护通知。
>
> **Non-Function requeirment:**
>
> > - Availiblity: 
> > - Scalability: 
> > - Performance: latency, throughput
> > - Security保护用户支付信息和设备的安全，防止欺诈和盗窃。
> > - Reliability 确保交易数据的一致性和可靠性，避免数据丢失或支付失败。
>
> **Extended requirement:**
>
> > - User are able to return the kindle?
>
> **Capacity Estimation and Constraints**
>
> > how many users we have? 
> > Let’s assuem we have 200M users.  let’s suppose each user will  generate 0.1 
> >
> > - Write/Read QPS
> >   - Average: 
> >   - Peak: 
>
> **Service:**
>
> > - **用户界面服务**：提供用户与售货机交互的界面，包括选择设备和支付操作。
> >
> > - **支付服务**：处理用户的支付请求，并验证支付信息。
> > - **库存管理服务**：管理设备库存信息，实时更新库存状态。
> > - **设备控制服务**：控制售货机的硬件操作，包括设备的发放和库存检测。
> > - **通知服务**：向用户发送购买凭证或电子收据，向管理员发送库存和维护通知。
> > - **监控和日志服务**：监控售货机的运行状态，记录操作日志和异常处理。
> > - **安全服务**：提供数据加密、身份验证和授权，保护用户信息和交易安全。
> >
> > **How do we detect and prevent abuse?** 
> > For instance, any service can put us out of business by consuming all our keys in the current design. To prevent abuse, we can limit users through their APIKey, how many URL they can create or access in a certain time.
>
> **Database:**
>
> > - **关系型数据库**：用于存储用户信息、支付记录、库存信息和设备信息。
> >
> >   > 1. 用户表 (Users Table)
> >   >
> >   >    > - user_id BIGINT PRIMARY KEY AUTO_INCREMENT,    
> >   >    > - username VARCHAR(255) NOT NULL UNIQUE,    
> >   >    > - password_hash VARCHAR(255) NOT NULL,    
> >   >    > - email VARCHAR(255) NOT NULL UNIQUE,    
> >   >    > - phone_number VARCHAR(20),    
> >   >    > - created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
> >   >
> >   > 2. Device talbe
> >   >
> >   >    > - device_id BIGINT PRIMARY KEY AUTO_INCREMENT,    
> >   >    > - model VARCHAR(255) NOT NULL,    
> >   >    > - price DECIMAL(10, 2) NOT NULL,    
> >   >    > - stock INT NOT NULL,    
> >   >    > - last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
> >   >
>
> **Component Detail Design:**
>
> > 1. 如何处理库存管理和设备售罄的情况？
> >
> >    > 库存管理服务实时监控设备库存，当库存接近售罄时，系统会提前通知管理员补充库存。如果设备售罄，用户界面会提示用户设备售罄并建议选择其他型号或稍后再试。
> >
> > 2. 如何确保支付的安全性？
> >
> >    > 支付服务通过SSL/TLS加密传输支付信息，使用支付网关处理支付请求，并采用双因素认证来验证用户身份。同时，支付记录表中存储支付状态，确保支付数据的一致性和安全性。
> >
> > 3. ：如何处理设备的远程管理和维护？
> >
> >    > 管理员可以通过远程管理接口监控售货机的运行状态，查看库存信息和维护记录。系统会自动生成维护通知，当设备需要维护时通知管理员进行处理。
> >
> > 4. 如何处理设备故障和异常情况？
> >
> >    > 监控服务实时监控售货机的运行状态，记录异常日志。当设备发生故障时，系统会自动生成故障报告并通知管理员进行修复。同时，可以设置备用机制，如启用其他售货机继续提供服务。
>
> **Scalability**
>
> > - **水平扩展**：通过增加更多的售货机，实现系统的水平扩展。
> > - **负载均衡**：使用负载均衡器将用户请求分配到不同的售货机和后台服务，提高系统的处理能力和可靠性。
> > - **数据库分片**：将数据库按照一定规则进行分片，提高数据库的读写性能和扩展能力。
> > - **缓存机制**：使用缓存（如Redis或Memcached）存储常用数据，减少数据库查询压力，提高系统响应速度。
> > - **异步处理**：对于不需要立即处理的任务（如支付验证和维护通知），使用消息队列（如RabbitMQ或Kafka）进行异步处理，提高系统的处理效率。
> > - **监控和报警**：使用监控工具（如Prometheus和Grafana）实时监控系统的各项指标，设置报警机制，及时发现和处理异常情况。

### 3.  Design Amazon Lockers

> Functional requirement: **
>
> > - Customers are able to find list of lockers based on the locations 
> > - Customres should be able to track the status of their package and receive notifications when it's ready for pickup.
> > - Customrers should be able to  retrieve their package securely, upon arrival at the locker location, .
> > - Deliveryman should be able to securely place packages into designated lockers.
> 
>**Non-Function requeirment:**
> 
>> - **Scalability**: The system should be scalable to handle a large number of users and packages, especially during peak times.
> > - **Availiblity**: high availability to ensure users can access lockers and retrieve their packages at any time.
> > - **Performance**: latency, throughput. It should provide fast response times for package pickup and notification delivery.
> > - **Security**:Security measures should be implemented to protect user identity and package integrity.
> 
>**Extended requirement:**
> 
>> 
> 
>**Capacity Estimation and Constraints**
> 
>> how many users we have? 
> > Let’s assuem we have 200M users.  let’s suppose 10% will use Amazon locker,  only 0.1 per each day, means a user use 0.01 Amzone lockers.
> >
> > - Write QPS
> >   - Average: 200,000,000* 0.1 * 0.1 * / (24*3600) = 23.14 QPS , Let’s say 30 QPS
> >   - Peak: 20*2 = 60 QPS 
> > - Read QPS（10 times of write）
> >   - Average: 300 QPS
> >   - Peak: 600 QPS
> > - Storage(WRITE)
> >   - 100 byte for every record, 30 * 86400 * 0.1 KB= 259,200 KB = 260 MB = 0.26 GB / DAY
> >   - 260GB distk can use 3 years
> > - Memory(READ)
> >   - Let’s follow 8/2 rule, means 20% of data taking 80% traffic, and we can cache 20% data per day
> >   - Every day, we need 300 * 86400 * 0.1KB * 0.2 =  0.52GB /DAY
> 
>**Service:**
> 
>> - Authentication service
> > - Notification service: notic customer once the package is ready for pickup
> > - Package tracking service
> > - Locker service
> >   - searchLockers(apiKey, userInfo, order, location)
> >   - bookLockers(apiKey, userInfo, order,  locker)
> >   - updateLockers(apiKey, userInfo, order,  locker) may not need
> >
> > - Code gerneration service
> >   - generateCode(apiKey, userInfo, order, locker)
> >   - validateCode(lockerStation, code)
> >   - closeLocer(apiKey, userInfo,  locker)
> >
> >
> > **How do we detect and prevent abuse?** 
> > For instance, any service can put us out of business by consuming all our keys in the current design. To prevent abuse, we can limit users through their APIKey.
> 
>**Database:**
> 
>> - SQL vs Nosql
> >
> >   > ACID requied
> >   >
> >   > less data
> >
> > - Schema
> >
> >   > - Locker:
> >   >
> >   >   > locker_id:  (Primary Key)
> >   >   >
> >   >   > location: varchar
> >   >   >
> >   >   > capacity: varchar
> >   >   >
> >   >   > availability boolean
> >   >
> >   > - Code
> >   >
> >   >   > code_id: int PK
> >   >   >
> >   >   > order_id: int 
> >   >   >
> >   >   > locker_id: int 
> 
>**Component Detail Design:**
> 
>> use unique order id to generate the code
> 
>**Scalability**
> 
>> - Database Replication:  Single leader replication read/write spliting 
> > - Cache redis 
> 

### 4.  Typeahead Suggestion

> **Typeahead suggestions enable users to search for known and frequently searched terms. As the user types into the search box, it tries to predict the query based on the characters the user has entered and gives a list of suggestion to complete the query. Typeahead suggestions help user to articulate their search queries better. It’s not about speeding up the search process but rather about guiding the users and lending them a helping hand in constructing their search query.**
>
> **Functional requirement: **
>
> > - When login user type the characters, the system will return the top 10th items based on his history
> >
> > - When unloginc user type the characters, the system will retrun the top 10th items based on the golbal search time.
> >
> > - We should log the user’s queries, and track them to update the trie structure offline
> >
> >   - Option1: We can make a copy of the trie on each server to update it offline. Once done we can switch to start using it and discard the old one.
> >
> >   - Option2:  We can have a master-slave configuration for each trie server. We can update slave
> >
> >     while the master is serving traffic. Once the update is complete, we can make the slave our new
> >
> >     master. We can later update our old master, which can then start serving traffic too.
>
> **Non-Function requeirment:**
>
> > - Availiblity: 
> > - Scalability: 
> > - Performance: 
> >   - latency： response time will be 20ms
> >   - throughput. 
> > - Security
>
> **Extended requirement:**
>
> > - 
>
> **Capacity Estimation and Constraints**
>
> > how many users we have? 
> > Let’s assuem we have 200M users.  let’s suppose each user will  search 1 times, and each time will type 20 characters
> >
> > - Write QPS
> >   - Average: 
> >   - Peak: 
> > - Read QPS 
> >   - Average: 200,000,000 * 1 * 20 / 24 / 3600 = 46296 = 46K QPS
> >   - Peak: 92K QPS
> > - Storage(WRITE)
> >   - 2 byte for every character, 200,000,000 * 0.002 KB * 20 = 8,000,000 KB = 8,000 MB = 8 GB / DAY
> >   -  If we assume we have 2% new queries every day and if we are maintaining our index for last one year
> >     8GB + (0.02 * 8GB * 365 days) = 66.4GB
> > - Memory(READ)
> >   - Let’s put all data into the cache
> >   - Every day, we need 200M * 0.5 KB* 10 *0.2 = 200,000,000 KB = 200GB /DAY
>
> **Service:**
>
> > - Log service
> >
> >   - use Flume + Kafaka + Spark streaming in HDFS and store data into NoSQL like Cassandra or Hbase
> >
> > - Trie service
> >
> >   > ```java
> >   > class Trie {
> >   > 
> >   >         // Trie 节点的定义
> >   >         class Node {
> >   >             boolean isWord = false;
> >   >           	double frequency;
> >   >             List<Node> children = Arrays.asList(new Node[26]);
> >   >         };
> >   >         Node Root, curr;
> >   >         // 初始化堆，按照出现频率从小到大排列
> >   >             Queue<Node> resultBuffer = new PriorityQueue<>(
> >   >                     (n1, n2) -> count.get(n2.frequency) - count.get(n1.frequency));
> >   > 
> >   >         // 运行一个深度优先搜索（DFS）在 Trie 上，从给定前缀开始，并将所有单词添加到 resultBuffer 中，限制结果大小为 10
> >   >         void dfsWithPrefix(Node curr, String word) {
> >   >             if (resultBuffer.size() == 10)
> >   >                 return;
> >   >             if (curr.isWord)
> >   >                 resultBuffer.add(word);
> >   > 
> >   >             // 在所有可能的路径上运行 DFS。
> >   >             for (char c = 'a'; c <= 'z'; c++)
> >   >                 if (curr.children.get(c - 'a') != null)
> >   >                     dfsWithPrefix(curr.children.get(c - 'a'), word + c);
> >   >         }
> >   >         Trie() {
> >   >             Root = new Node();
> >   >         }
> >   > 
> >   >         // 在 Trie 中插入字符串
> >   >         void insert(String s) {
> >   > 
> >   >             // 将 curr 指针指向 Trie 的根节点。
> >   >             curr = Root;
> >   >             for (char c : s.toCharArray()) {
> >   >                 if (curr.children.get(c - 'a') == null)
> >   >                     curr.children.set(c - 'a', new Node());
> >   >                 curr = curr.children.get(c - 'a');
> >   >             }
> >   > 
> >   >             // 将该节点标记为一个完成的单词。
> >   >             curr.isWord = true;
> >   >         }
> >   > 
> >   >         // 获取以指定前缀开头的单词
> >   >         List<String> getWordsStartingWith(String prefix) {
> >   >             curr = Root;
> >   >             resultBuffer = new ArrayList<String>();
> >   >             // 将 curr 移动到其 Trie 表示中前缀的末尾。
> >   >             for (char c : prefix.toCharArray()) {
> >   >                 if (curr.children.get(c - 'a') == null)
> >   >                     return resultBuffer;
> >   >                 curr = curr.children.get(c - 'a');
> >   >             }
> >   >             dfsWithPrefix(curr, prefix);
> >   >             return resultBuffer;
> >   >         }
> >   >     };
> >   > 
> >   > List<List<String>> suggestedProducts(String[] products, String searchWord) {
> >   >             Trie trie = new Trie();
> >   >             List<List<String>> result = new ArrayList<>();
> >   >             // 将所有单词添加到 Trie 中。
> >   >             for (String w : products)
> >   >                 trie.insert(w);
> >   >             String prefix = new String();
> >   >             for (char c : searchWord.toCharArray()) {
> >   >                 prefix += c;
> >   >                 result.add(trie.getWordsStartingWith(prefix));
> >   >             }
> >   >             return result;
> >   >         }
> >   > ```
> >
> > - RESTful API
> >
> > **How do we detect and prevent abuse?** 
> > For instance, any service can put us out of business by consuming all our keys in the current design. To prevent abuse, we can limit users through their APIKey, how many URL they can create or access in a certain time.
>
> **Database:**
>
> > - Files system: store Trie structure
> > - NoSql database used to store log.
>
> **Component Detail Design:**
>
> > - Query  service Optimize 
> >
> >   > 1. **How to optimize the search speed?**
> >   >
> >   >    - We can merge nodes that have only one branch to save storage space and search 
> >   >    - We  can store top suggestions with each node. To save the space, we can optimize our storage by storing only references of the terminal nodes rather than storing the entire
> >   >
> >   > 2. **How to update trie?**
> >   >
> >   >    - Update the slave then sync it.
> >   >
> >   >    - We can update the tire offline
> >   >    - update the frequencies of typeahead suggestions, give more weight to the latest data.
> >   >    - Update the top 10 frequesce  on node.
> >   >
> >   > 3. **What could be different ranking criteria for suggestions?** 
> >   >
> >   >    - In addition to a simple count, for terms ranking
> >   >    - we have to consider other factors too, e.g., freshness, user location, language, demographics, personal history etc.
> >
> > - Log service Optimize
> >
> >   > 1. use spark in hdfs 
> >
> > - Frontend optimize
> >
> >   > 1. The client should only try hitting the server if the user has not pressed any key for 50ms.
> >   > 2. If the user is constantly typing, the client can cancel the in-progress requests.
> >   > 3. Initially, the client can wait until the user enters a couple of characters.
> >   > 4. Clients can pre-fetch some data from the server to save future requests.
> >   > 5. Clients can store the recent history of suggestions locally. Recent history has a very high rate of being
> >   >
> >   > reused.
> >   >
> >   > 6. Establishing an early connection with server turns out to be one of the most important factors. As
> >   >
> >   > soon as the user opens the search engine website, the client can open a connection with the server.
> >   >
> >   > So when user types in the first character, client doesn’t waste time in establishing the connection.
> >   >
> >   > 7. The server can push some part of their cache to CDNs and Internet Service Providers (ISPs) for
> >   >
> >   > efficiency.
>
> **Scalability**
>
> > - Data partition, partiton into multiple server
> >
> >   > 1. ** Range Based Partitioning**
> >   > 2. **Partition based on the maximum capacity of the server**
> >   > 3. **Partition based on the hash of the term**
> >
> > - Cache
> >
> >   > 1. We can have separate cache servers in front of the trie servers, holding most frequently searched terms and their typeahead suggestions.
> >   >2. CDN for unlogin user
> >   
> >- Replication and load balance
> > 
> >  > We should have replicas for our trie servers both for load balancing and also for fault tolerance. We also need
> >   >
> >   > a load balancer that keeps track of our data partitioning scheme and redirects traffic based on the prefixes.

### 5. Design distributed task scheduler

**Functional requirement: **

> - 用户可以创建、查看、编辑和删除任务。
> - 用户可以设置任务的执行时间和频率（如定时任务）。
> - 系统需要处理任务的分发和执行。
> - 系统需要支持任务的状态跟踪（如待执行、进行中、已完成、失败）。
> - 系统需要支持任务执行失败后的重试机制。
> - 系统需要提供任务日志以便于调试和审计。

**Non-Function requeirment:**

> - Availiblity: 
> - Scalability: 
> - Performance: latency, throughput
> - Security保护用户数据，确保只有授权用户可以操作任务。
> - Reliability 确保任务数据的一致性和可靠性，避免任务丢失或重复执行。

**Extended requirement:**

> - 

**Capacity Estimation and Constraints**

> how many users we have? 
> Let’s assuem we have 2M users.  let’s suppose each user will  run 1 time every day
>
> - Write QPS
>   - Average: 2000,000* 0.1 / (24*3600) = 2.314 QPS , Let’s say 2 QPS
>   - Peak: 3*2 = 6QPS 
> - Read QPS (100 times of write)
>   - Average: 30K QPS
>   - Peak: 60K QPS
> - Storage(WRITE)
>   - 500 byte for every url, 2,000,000* 0.1* 0.5 KB= 100,000 KB = 100 MB = 0.1 GB / DAY
>   - 0.10 TB distk can use 3 years, if we store 5 years, we need 20TB including backup
> - Memory(READ)
>   - 

**Service:**

> - **用户管理服务**：处理用户注册、登录、身份验证和权限管理。
>
> - **任务管理服务**：处理任务的创建、查看、编辑和删除。
> - **调度服务**：负责任务的定时调度和分发。
> - **执行服务**：实际执行分配的任务并反馈状态。
> - **监控和日志服务**：监控系统性能，记录任务执行日志和异常处理。
> - **通知服务**：发送任务状态更新和错误通知。
>
> **How do we detect and prevent abuse?** 
> For instance, any service can put us out of business by consuming all our keys in the current design. To prevent abuse, we can limit users through their APIKey, how many URL they can create or access in a certain time.

**Database:**

> - **关系型数据库**：用于存储用户信息、任务信息和任务日志。
>
>   > 1. user table
>   >
>   >    > - user_id BIGINT PRIMARY KEY AUTO_INCREMENT,    
>   >    > - username VARCHAR(255) NOT NULL UNIQUE,    
>   >    > - password_hash VARCHAR(255) NOT NULL,    
>   >    > - email VARCHAR(255) NOT NULL UNIQUE,    
>   >    > - created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
>   >
>   > 2. Task table
>   >
>   >    > - task_id BIGINT PRIMARY KEY AUTO_INCREMENT,    
>   >    > - user_id BIGINT NOT NULL,    
>   >    > - task_name VARCHAR(255) NOT NULL,    
>   >    > - description TEXT,    schedule_time TIMESTAMP NOT NULL,    
>   >    > - frequency VARCHAR(50),    
>   >    > - status VARCHAR(50) DEFAULT 'PENDING',    
>   >    > - created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,    
>   >    > - updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,   
>   >    > - FOREIGN KEY (user_id) REFERENCES Users(user_id)
>
> - **NoSQL数据库**：用于存储实时任务状态和任务执行结果，以提高读取性能。
>
>   > 1. Tasklogs talbe
>   >
>   >    > - log_id BIGINT PRIMARY KEY AUTO_INCREMENT,    
>   >    > - task_id BIGINT NOT NULL,    
>   >    > - status VARCHAR(50),    
>   >    > - message TEXT,    
>   >    > - created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

**Component Detail Design:**

> - Task Manage service
>
>   > 1. 如何确保任务的高可用性和不重复执行？
>   >
>   >    > 可以使用分布式锁机制（如基于Redis或ZooKeeper）来确保任务调度时的互斥性，避免多个调度器同时分发同一任务。同时，任务状态需要原子更新，以确保任务不重复执行。
>   >
>   > 2. 如何处理任务执行失败后的重试机制？
>   >
>   >    > 可以在任务表中增加重试次数和重试间隔字段，当任务执行失败时，根据预设的重试策略重新调度任务。在重试时，可以设置最大重试次数，避免无限重试。
>   >
>   > 3. 如何设计一个高效的任务调度算法？
>   >
>   >    > 任务调度算法可以基于优先级和时间窗口来设计。优先调度紧急和即将到期的任务，采用轮询或权重分配的方式将任务分发给空闲的执行节点。此外，可以使用队列机制（如RabbitMQ或Kafka）来管理任务的分发和执行。
>   >
>   > 4. 如何监控和日志记录任务的执行情况？
>   >
>   >    > 可以通过监控服务和日志服务，实时监控任务的执行情况。每个任务执行时，记录其开始时间、结束时间、执行状态和日志信息。使用集中式日志管理系统（如ELK Stack）来收集和分析日志数据。

**Scalability**

> - **水平扩展**：通过增加更多的调度器和执行节点，实现系统的水平扩展。
> - **负载均衡**：使用负载均衡器将任务分发到不同的执行节点，提高系统的处理能力和可靠性。
> - **数据库分片**：将数据库按照一定规则进行分片，提高数据库的读写性能和扩展能力。
> - **缓存机制**：使用缓存（如Redis或Memcached）存储常用数据，减少数据库查询压力，提高系统响应速度。
> - **异步处理**：对于不需要立即处理的任务（如日志记录和通知），使用消息队列（如RabbitMQ或Kafka）进行异步处理，提高系统的处理效率。
> - **监控和报警**：使用监控工具（如Prometheus和Grafana）实时监控系统的各项指标，设置报警机制，及时发现和处理异常情况。

### 6. Design Auction System

> Design a auction system that artist can put his painting， bidder can raise the price to gaint the painting
>
> **Functional requirement: **
>
> > - 用户可以创建拍卖，设定起拍价、拍卖时长和其他规则。
> > - 用户可以浏览和搜索拍卖商品。
> > - 用户可以参与竞拍，对拍卖商品出价。
> > - 系统需要实时更新最高出价并通知相关用户。
> > - 拍卖结束时，系统需要确定赢家并通知他们。
> > - 用户可以查看他们的竞拍历史和当前的竞拍状态。
> > - 支持拍卖商品的支付和结算。
> > - 管理员可以管理和监控拍卖活动。
>
> **Non-Function requeirment:**
>
> > - Availiblity: 
> > - Scalability: 
> > - Performance: latency, throughput
> > - Security保护用户信息和竞拍数据，防止欺诈和恶意竞拍。
> > - Realiability 确保拍卖数据的一致性和可靠性，避免数据丢失或出价错误
> > - ACID 确保竞拍过程中的数据一致性，避免竞拍冲突。
>
> **Extended requirement:**
>
> > - 
>
> **Capacity Estimation and Constraints**
>
> > how many users we have? 
> > Let’s assuem we have 200M users.  let’s suppose each user will  generate 0.1 URL per day， redirected by 10 URLs/day
> >
> > - Write QPS
> >   - Average: 200,000,000* 0.1 / (24*3600) = 231.4 QPS , Let’s say 300 QPS
> >   - Peak: 300*2 = 600 QPS 
> > - Read QPS (100 times of write)
> >   - Average: 30K QPS
> >   - Peak: 60K QPS
> > - Storage(WRITE)
> >   - 500 byte for every url, 200,000,000* 0.1* 0.5 KB= 10,000,000 KB = 10000 MB = 10 GB / DAY
> >   - 10 TB distk can use 3 years, if we store 5 years, we need 20TB including backup
> > - Memory(READ)
> >   - Let’s follow 8/2 rule, means 20% of URLs taking 80% traffic, and we can cache 20% URLs per day
> >   - Every day, we need 200M * 0.5 KB* 10 *0.2 = 200,000,000 KB = 200GB /DAY
>
> **Service:**
>
> > - **用户管理服务**：处理用户注册、登录、身份验证和权限管理。
> >
> > - **Bidding management拍卖管理服务**：处理拍卖的创建、查看、编辑和删除。
> > - **Bidding service竞拍服务**：处理用户的竞拍请求，并实时更新出价信息。
> > - **通知服务**：实时通知用户出价更新和拍卖结果。
> > - **支付服务**：处理竞拍结束后的支付和结算。
> > - **监控和日志服务**：监控系统性能，记录操作日志和异常处理。
> > - **安全服务**：提供数据加密、身份验证和授权，保护用户信息和交易安全。
> >
> > **How do we detect and prevent abuse?** 
> > For instance, any service can put us out of business by consuming all our keys in the current design. To prevent abuse, we can limit users through their APIKey, how many URL they can create or access in a certain time.
>
> **Database:**
>
> > - **关系型数据库**：用于存储用户信息、拍卖信息和竞拍记录。
> >
> >   > 1. User table
> >   >
> >   >    > - user_id BIGINT PRIMARY KEY AUTO_INCREMENT,    
> >   >    > - username VARCHAR(255) NOT NULL UNIQUE,    
> >   >    > - password_hash VARCHAR(255) NOT NULL,   
> >   >    > -  email VARCHAR(255) NOT NULL UNIQUE,    
> >   >    > - phone_number VARCHAR(20),    
> >   >    > - created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
> >   >
> >   > 2. Auctions table
> >   >
> >   >    > - auction_id BIGINT PRIMARY KEY AUTO_INCREMENT,    
> >   >    > - user_id BIGINT NOT NULL,    
> >   >    > - item_name VARCHAR(255) NOT NULL,    
> >   >    > - description TEXT,    
> >   >    > - start_price DECIMAL(10, 2) NOT NULL,    
> >   >    > - current_price DECIMAL(10, 2),    
> >   >    > - start_time TIMESTAMP NOT NULL,    
> >   >    > - end_time TIMESTAMP NOT NULL,    
> >   >    > - status VARCHAR(50) DEFAULT 'ONGOING',    
> >   >    > - created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,    
> >   >    > - updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,    
> >   >    > - FOREIGN KEY (user_id) REFERENCES Users(user_id)
> >   >
> >   > 3. Bid table
> >   >
> >   >    > - bid_id BIGINT PRIMARY KEY AUTO_INCREMENT,    
> >   >    > - auction_id BIGINT NOT NULL,    user_id BIGINT NOT NULL,   
> >   >    > - bid_amount DECIMAL(10, 2) NOT NULL,    
> >   >    > - bid_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,    
> >   >    > - FOREIGN KEY (auction_id) REFERENCES Auctions(auction_id),    FOREIGN KEY (user_id) REFERENCES Users(user_id)
> >
> > - **NoSQL数据库**：用于存储实时的出价信息和拍卖状态缓存，以提高读取性能。
>
> **Component Detail Design:**
>
> > 1. ：如何确保竞拍过程中的数据一致性？
> >
> >    > 可以通过数据库事务和乐观锁机制确保竞拍过程中的数据一致性。在出价时，使用事务锁定拍卖记录，确保每次只有一个出价能成功更新当前价格。
> >
> > 2. 如何处理竞拍结束后的支付和结算？
> >
> >    > 在拍卖结束后，系统会通知赢家并生成支付订单。支付服务负责处理支付请求，验证支付信息，并在支付成功后更新拍卖状态和付款记录。
> >
> > 3. 如何设计一个高效的通知服务？
> >
> >    > 通知服务可以使用消息队列（如RabbitMQ或Kafka）处理通知的发送和用户响应。每条通知包含用户ID、拍卖ID和出价信息，用户响应后更新通知状态并执行相应操作。
> >
> > 4. 如何处理恶意竞拍和欺诈行为？
> >
> >    > 可以通过用户行为分析和风控系统检测异常出价和恶意行为，及时冻结可疑用户账号并通知管理员进行处理。同时，使用多因素认证和支付验证机制提高交易安全性。
>
> **Scalability**
>
> > - **水平扩展**：通过增加更多的服务器节点和数据库实例，实现系统的水平扩展。
> > - **负载均衡**：使用负载均衡器将用户请求分配到不同的服务器，提高系统的处理能力和可靠性。
> > - **数据库分片**：将数据库按照一定规则进行分片，提高数据库的读写性能和扩展能力。
> > - **缓存机制**：使用缓存（如Redis或Memcached）存储常用数据，减少数据库查询压力，提高系统响应速度。
> > - **异步处理**：对于不需要立即处理的任务（如通知发送和支付结算），使用消息队列（如RabbitMQ或Kafka）进行异步处理，提高系统的处理效率。
> > - **监控和报警**：使用监控工具（如Prometheus和Grafana）实时监控系统的各项指标，设置报警机制，及时发现和处理异常情况。

### 6. Design 订阅 System

- design是设计一个订阅系统，主要就一个use case，如果对订阅的东西有更新，需要通知相关user，得到用户许可后才能给他更新，不然就cancel他的订阅。这轮感觉答的不好，面试官在面试中也没有什么反馈，最后还剩15分钟就说问完了。有点草草了事的意思。

> **Functional requirement: **
>
> > - 用户可以订阅某些内容（如电子书、软件、服务等）。
> > - 当订阅的内容有更新时，系统需要通知相关用户。
> > - 用户需要明确同意更新，才能进行更新操作。
> > - 如果用户拒绝更新或未响应，系统需要取消该用户的订阅。
> > - 用户可以查看其订阅的状态和历史记录。
> > - 用户可以随时取消订阅。
>
> **Non-Function requeirment:**
>
> > - Availiblity: 系统需要在大部分时间保持可用，避免宕机。
> > - Scalability: 系统应能处理不断增加的用户和订阅请求
> > - Performance: latency, throughput
> > - Security 保护用户数据和订阅内容的隐私和安全。
>
> **Extended requirement:**
>
> > - 
>
> **Capacity Estimation and Constraints**
>
> > how many users we have? 
> > Let’s assuem we have 200M users.  
> >
> > - Notice QPS
> >   - Average: 200,000,000* 1 / (24*3600) = 2314 QPS , Let’s say 3000 QPS
> >   - Peak: 3000*2 = 6000 QPS 
> > - Storage(WRITE)
> >   - 500 byte for every url, 200,000,000* 0.1* 0.5 KB= 10,000,000 KB = 10000 MB = 10 GB / DAY
> >   - 10 TB distk can use 3 years, if we store 5 years, we need 20TB including backup
> > - Memory(READ)
> >   - Let’s follow 8/2 rule, means 20% of URLs taking 80% traffic, and we can cache 20% URLs per day
> >   - Every day, we need 200M * 0.5 KB* 10 *0.2 = 200,000,000 KB = 200GB /DAY
>
> **Service:**
>
> > - **用户管理服务**：处理用户注册、登录、身份验证和权限管理。
> >
> > - **订阅管理服务**：处理订阅的创建、更新、取消和查看。
> > - **通知服务**：发送订阅内容更新的通知，并处理用户的同意或拒绝。
> > - **内容管理服务**：管理订阅内容及其更新。
> > - **监控和日志服务**：监控系统性能，记录操作日志和异常处理。
> > - **安全服务**：提供数据加密、身份验证和授权。
> >
> > **How do we detect and prevent abuse?** 
> > For instance, any service can put us out of business by consuming all our keys in the current design. To prevent abuse, we can limit users through their APIKey, how many URL they can create or access in a certain time.
>
> **Database:**
>
> > - **关系型数据库**：用于存储用户信息、订阅信息和内容更新记录。
> >
> >   > 1. User table
> >   >
> >   >    > - user_id BIGINT PRIMARY KEY AUTO_INCREMENT,    
> >   >    > - username VARCHAR(255) NOT NULL UNIQUE,    
> >   >    > - password_hash VARCHAR(255) NOT NULL,    
> >   >    > - email VARCHAR(255) NOT NULL UNIQUE,    
> >   >    > - created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
> >   >
> >   > 2. Subscriptions table
> >   >
> >   >    > - subscription_id BIGINT PRIMARY KEY AUTO_INCREMENT,    
> >   >    > - user_id BIGINT NOT NULL,    
> >   >    > - content_id BIGINT NOT NULL,    
> >   >    > - status VARCHAR(50) DEFAULT 'ACTIVE',    
> >   >    > - created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,    
> >   >    > - updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,    
> >   >    > - FOREIGN KEY (user_id) REFERENCES Users(user_id)
> >
> > - **NoSQL数据库**：用于存储实时订阅状态和通知信息，以提高读取性能。
> >
> >   > 1. Content table
> >   >
> >   >    > - content_id BIGINT PRIMARY KEY AUTO_INCREMENT,    
> >   >    > - title VARCHAR(255) NOT NULL,    
> >   >    > - description TEXT,    
> >   >    > - version INT DEFAULT 1,    
> >   >    > - updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
> >   >
> >   > 2. UpdateNotifications table
> >   >
> >   >    > - notification_id BIGINT PRIMARY KEY AUTO_INCREMENT,   
> >   >    > - subscription_id BIGINT NOT NULL,   
> >   >    > - content_id BIGINT NOT NULL,    
> >   >    > - status VARCHAR(50) DEFAULT 'PENDING',    
> >   >    > - created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,    
> >   >    > - FOREIGN KEY (subscription_id) REFERENCES Subscriptions(subscription_id),    FOREIGN KEY (content_id) REFERENCES Content(content_id)
>
> **Component Detail Design:**
>
> > - notice service
> >
> >   > 1. 如何确保用户明确同意更新？
> >   >
> >   >    > 可以通过通知服务发送更新请求，用户必须在通知中明确点击“同意”按钮才能执行更新操作。在数据库中记录用户的同意操作，以备后续审计。
> >   >
> >   > 2. 如何处理用户未响应更新通知的情况？
> >   >
> >   >    > 在发送通知后，可以设置一个定时任务检查用户是否在一定时间内响应。如果超过时间未响应，自动取消该用户的订阅，并通知用户订阅已取消。
> >   >
> >   > 3. 如何设计通知服务？
> >   >
> >   >    > 通知服务可以使用消息队列（如RabbitMQ或Kafka）处理通知的发送和用户响应。每条通知包含用户ID、订阅ID、内容ID和状态，用户响应后更新通知状态并执行相应操作。
> >   >
> >   > 4. 如何确保系统的高可用性和可扩展性？
> >   >
> >   >    > 可以通过使用负载均衡器将流量分配到多个服务器节点，采用数据库复制和分片技术，使用容灾备份和自动故障切换机制，确保系统的高可用性和可扩展性。
>
> **Scalability**
>
> > - **水平扩展**：通过增加更多的服务器节点和数据库实例，实现系统的水平扩展。
> > - **负载均衡**：使用负载均衡器将流量分配到多个服务器，提高系统的处理能力和可靠性。
> > - **数据库分片**：将数据库按照一定规则进行分片，提高数据库的读写性能和扩展能力。
> > - **缓存机制**：使用缓存（如Redis或Memcached）存储常用数据，减少数据库查询压力，提高系统响应速度。
> > - **异步处理**：对于不需要立即处理的任务（如通知发送），使用消息队列（如RabbitMQ或Kafka）进行异步处理，提高系统的处理效率。
> > - **监控和报警**：使用监控工具（如Prometheus和Grafana）实时监控系统的各项指标，设置报警机制，及时发现和处理异常情况。



### 7. Design UBER

> Functional requirement: **
>
> > - Driver report locations
> > - Rider request Uber, match a driver with rider
> > - Driver deny / accept a request
> > - Driver cancel a matched request
> > - Rider cancel a request
> > - Driver pick up a rider / start a trip
> > - Driver drop off a rider / end a trip
>
> **Non-Function requeirment:**
>
> > - Availiblity: 
> > - Scalability: 
> > - Performance: latency, throughput
> > - Security
>
> **Extended requirement:**
>
> > - Uber Pool
> > - Uber eat
>
> **Capacity Estimation and Constraints**
>
> > how many users we have? 
> > Let’s assuem we have 200M users.  假设同时在线的司机平均约为 600k(猜的)
> >
> > - Driver QPS, Driver report locations by every 4 seconds
> >   - Average Driver QPS = 600k / 4 = 150k
> >   - Peak:300k QPS 
> > - Rider QPS
> >   - Average: 无需随时汇报位置, 一定远小于Driver QPS
> > - Storage(WRITE)
> >   - 假如每条Location都记录:600 k * 86400 / 4 * 100bytes (每条位置记录)~ 1.3 T / 天
> >   - 假如只记录当前位置信息:600 k * 100 bytes = 60 M
> > - Memory(READ)
> >   - 
>
> **Service:**
>
> > ![image-20240519221021857](系统设计案例.assets/image-20240519221021857.png)
> >
> > - GeoService 记录车的位置
> >
> >   > `reportLocation(apiKey, driverId, location)`
> >
> > - DispatchService 匹配打车请求
> >
> >   > `findDriverNearBy(apiKey, riderId, riderLocation, tripInfo)`
> >
> > **How do we detect and prevent abuse?** 
> > For instance, any service can put us out of business by consuming all our keys in the current design. To prevent abuse, we can limit users through their APIKey, how many URL they can create or access in a certain time.
>
> **Database:**
>
> > - NoSQL
> >
> >   > 1. Location table
> >   >
> >   >    > ![image-20240519221452955](系统设计案例.assets/image-20240519221452955.png)
> >   >
> >   > 2. Trip table
> >   >
> >   >    > ![image-20240519221524029](系统设计案例.assets/image-20240519221524029.png)
> >   >
> >   > 3. geohash table
> >   >
> >   >    > • SQL 数据库
> >   >    >
> >   >    > > • CREATE INDEX on geohash; 
> >   >    > >
> >   >    > > • 使用 Like Query
> >   >    > >
> >   >    > > • SELECT * FROM location WHERE geohash LIKE` 9q9hv%`;
> >   >    >
> >   >    > • NoSQL - Cassandra
> >   >    >
> >   >    > >  • 将 geohash 设为 column key
> >   >    > >  • 使用 range query (9q9hv0, 9q9hvz)
> >   >    >
> >   >    > NoSQL - Redis
> >   >    >
> >   >    > > - Driver 的位置分级存储
> >   >    > >   - 如Driver的位置如果是9q9hvt，则存储在9q9hvt，9q9hv，9q9h这3个key中 
> >   >    > >   - 6位geohash的精度已经在一公里以内，对于Uber这类应用足够了
> >   >    > >   - 4位geohash的精度在20公里以上了，再大就没意义了，你不会打20公里以外的
> >   >    > > - key = 9q9hvt, value = set of drivers in this location
> >   >    >
> >   >    > 
>
> **Component Detail Design:**
>
> > - DispatchService
> >
> >   > 1. 如何存储和查询地理位置信息
> >   >
> >   >    > 解决思路:把二维映射到一维。
> >   >    >
> >   >    > - Google S2 （更精准，库函数API丰富）
> >   >    >
> >   >    >   > - Read more: http://bit.ly/1WgMpSJ
> >   >    >   > - Hilbert Curve: http://bit.ly/1V16HRa
> >   >    >   > - 将地址空间映射到2^64的整数
> >   >    >   > - 特性:如果两个一维整数比较接近，对应的二维坐标就比较接近
> >   >    >   > - Example: (-30.043800, -51.140220) → 10743750136202470315
> >   >    >
> >   >    > - Geohash(比较简单，准确度差一些)
> >   >    >
> >   >    >   > - Read more: http://bit.ly/1S0Qzeo 
> >   >    >   >
> >   >    >   > - Peano Curve
> >   >    >   >
> >   >    >   > - Base32:0-9, a-z 去掉 (a,i,l,o)
> >   >    >   >
> >   >    >   > - 为什么用 base32 ? 因为刚好 2^5 可以用 5 位二进制表示 
> >   >    >   >
> >   >    >   > -  核心思路二分法
> >   >    >   >
> >   >    >   > - 特性:公共前缀越长，两个点越接近
> >   >    >   >
> >   >    >   > -  Example: (-30.043800, -51.140220)
> >   >    >   >
> >   >    >   >   ![image-20240519222409728](系统设计案例.assets/image-20240519222409728.png)
> >   >
> >   > 2. **How can we efficiently implement Notification service?** 
> >   >
> >   >    > We can either use HTTP long polling or push notifications.
> >   >
> >   > 3. Rider pull infor to find near by user?
> >   >
> >   >    > Clients can send their current location, and the server will find all the nearby drivers from the **QuadTree** to return them to the client.
> >   >
> >   > 4. 如何打车
> >   >
> >   >    > ![image-20240519223442229](系统设计案例.assets/image-20240519223442229.png)
>
> **Scalability**
>
> > - Cache
> >
> >   > - Sharding by city
> >   >
> >   >   > use Geo Fence
> >   >
> >   > - Master Slave
> >   >
> >   >   > 每个 Master 下挂两个 Slave, Master 挂了 Slave 就顶上
> >
> > - NoSql
> >
> >   > 既然一定要用多台机器的话，我们可以用 1000 台 Cassandra / Riak 这样的 NoSQL 数据库，平均每台分摊 300 的 QPS 就不大了, 这类数据库会帮你更好的处理 Replica 和挂掉之后恢复的问题
> >
> > 



### 8. Design Amazon payment

> Functional requirement: **
>
> > - 
>
> **Non-Function requeirment:**
>
> > - Availiblity: 
> > - Scalability: 
> > - Performance: latency, throughput
> > - Security
>
> **Extended requirement:**
>
> > - 
>
> **Capacity Estimation and Constraints**
>
> > how many users we have? 
> > Let’s assuem we have 200M users.  let’s suppose each user will  generate 0.1 URL per day， redirected by 10 URLs/day
> >
> > - Write QPS
> >   - Average: 200,000,000* 0.1 / (24*3600) = 231.4 QPS , Let’s say 300 QPS
> >   - Peak: 300*2 = 600 QPS 
> > - Read QPS (100 times of write)
> >   - Average: 30K QPS
> >   - Peak: 60K QPS
> > - Storage(WRITE)
> >   - 500 byte for every url, 200,000,000* 0.1* 0.5 KB= 10,000,000 KB = 10000 MB = 10 GB / DAY
> >   - 10 TB distk can use 3 years, if we store 5 years, we need 20TB including backup
> > - Memory(READ)
> >   - Let’s follow 8/2 rule, means 20% of URLs taking 80% traffic, and we can cache 20% URLs per day
> >   - Every day, we need 200M * 0.5 KB* 10 *0.2 = 200,000,000 KB = 200GB /DAY
>
> **Service:**
>
> > - Functions
> >
> > - RESTful API
> >
> > **How do we detect and prevent abuse?** 
> > For instance, any service can put us out of business by consuming all our keys in the current design. To prevent abuse, we can limit users through their APIKey, how many URL they can create or access in a certain time.
>
> **Database:**
>
> > 
>
> **Component Detail Design:**
>
> > 
>
> **Scalability**
>
> > 



### 9. 系统设计餐厅预约

> Functional requirement: **
>
> > - 用户可以浏览餐厅列表和查看餐厅详细信息。
> > - 用户可以根据时间和日期预订餐厅座位。
> > - 用户可以查看和管理他们的预约。
> > - 用户可以取消或修改预约。
> > - 餐厅可以管理他们的座位和时间安排。
> > - 系统发送预约确认和提醒通知。
>
> **Non-Function requeirment:**
>
> > - Availiblity: 系统需要在大部分时间保持可用，避免宕机。
> > - Scalability: 系统应能处理不断增加的用户和预约请求。
> > - Performance: latency, throughput 系统应能快速响应用户请求，提供流畅的预订体验。
> > - Security: 保护用户数据和餐厅信息的隐私和安全。
> > - Reliability: 确保预约数据的一致性和可靠性，避免数据丢失或重复预订
>
> **Extended requirement:**
>
> > - 
>
> **Capacity Estimation and Constraints**
>
> > how many users we have? 
> > Let’s assuem we have 200M users.  let’s suppose each user will  book 0.1 restaurant per day
> >
> > - Write QPS
> >   - Average: 200,000,000* 0.1 / (24*3600) = 231.4 QPS , Let’s say 300 QPS
> >   - Peak: 300*2 = 600 QPS 
> > - Read QPS (2 times of write)
> >   - Average: 600 QPS
> >   - Peak: 1200 QPS
> > - Storage(WRITE)
> >   - 500 byte for every book, 200,000,000* 0.1* 0.5 KB= 10,000,000 KB = 10000 MB = 10 GB / DAY
> >   - 10 TB distk can use 3 years, if we store 5 years, we need 20TB including backup
> > - Memory(READ)
> >   - Let’s follow 8/2 rule, means 20% of URLs taking 80% traffic, and we can cache 20% URLs per day
> >   - Every day, we need 200M * 0.5 KB* 10 *0.2 = 200,000,000 KB = 200GB /DAY
>
> **Service:**
>
> > - **User service用户管理服务**：处理用户注册、登录、身份验证和权限管理。
> >
> > - **Restaurant Manage service餐厅管理服务**：管理餐厅信息、座位和时间安排。
> > - **Reservation service预约管理服务**：处理用户的预约请求、取消和修改操作。
> > - **Notice service通知服务**：发送预约确认、修改和提醒通知。
> > - **Search service搜索服务**：提供快速和准确的餐厅搜索功能。
> > - **安全服务**：提供数据加密、身份验证和授权。
> > - **监控和日志服务**：监控系统性能，记录操作日志和异常处理。
> >
> > **How do we detect and prevent abuse?** 
> > For instance, any service can put us out of business by consuming all our keys in the current design. To prevent abuse, we can limit users through their APIKey, how many URL they can create or access in a certain time.
>
> **Database:**
>
> > - **关系型数据库**：用于存储用户信息、餐厅信息、预约记录和座位安排
> >
> >   > 1. User table
> >   >
> >   >    > - user_id BIGINT PRIMARY KEY AUTO_INCREMENT,    
> >   >    > - username VARCHAR(255) NOT NULL UNIQUE,    
> >   >    > - password_hash VARCHAR(255) NOT NULL,    
> >   >    > - email VARCHAR(255) NOT NULL UNIQUE,    
> >   >    > - phone_number VARCHAR(20),    
> >   >    > - created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
> >   >
> >   > 2. Restaurant  table
> >   >
> >   >    > - restaurant_id BIGINT PRIMARY KEY AUTO_INCREMENT,   
> >   >    > -  name VARCHAR(255) NOT NULL,   
> >   >    > -  address VARCHAR(255) NOT NULL,    
> >   >    > - phone_number VARCHAR(20),    
> >   >    > - email VARCHAR(255),    
> >   >    > - description TEXT,    
> >   >    > - created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
> >   >
> >   > 3. Seat table
> >   >
> >   >    > - seat_id BIGINT PRIMARY KEY AUTO_INCREMENT,   
> >   >    > -  restaurant_id BIGINT NOT NULL,    
> >   >    > - seat_number INT NOT NULL,    
> >   >    > - capacity INT NOT NULL,    
> >   >    > - available BOOLEAN DEFAULT TRUE,    
> >   >    > - FOREIGN KEY (restaurant_id) REFERENCES Restaurants(restaurant_id)
> >   >
> >   > 4. Reservation table
> >   >
> >   >    > - reservation_id BIGINT PRIMARY KEY AUTO_INCREMENT,    
> >   >    > - user_id BIGINT NOT NULL,   
> >   >    > -  restaurant_id BIGINT NOT NULL,    
> >   >    > - seat_id BIGINT,   
> >   >    > -  reservation_time TIMESTAMP NOT NULL,   
> >   >    > -  status VARCHAR(50) DEFAULT 'PENDING',    
> >   >    > - created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,    
> >   >    > - FOREIGN KEY (user_id) REFERENCES Users(user_id),    
> >   >    > - FOREIGN KEY (restaurant_id) REFERENCES Restaurants(restaurant_id),    
> >   >    > - FOREIGN KEY (seat_id) REFERENCES Seats(seat_id)
> >
> > - **NoSQL数据库**：用于存储实时预约数据和餐厅信息缓存，以提高读取性能。
>
> **Component Detail Design:**
>
> > - Book Service
> >
> >   > 1. 如何设计座位管理功能？
> >   >
> >   >    > 座位管理功能可以通过座位表来实现，每个餐厅有一个座位表记录座位编号、容量和可用状态。餐厅管理服务可以更新座位状态，用户预约时系统会检查座位的可用性并更新预约记录。
> >   >
> >   > 2. 如何处理预约冲突？
> >   >
> >   > **回答**：在处理预约请求时，我们可以使用事务机制确保操作的原子性，避免并发请求导致的预约冲突。在数据库级别，可以设置唯一索引或锁定相关记录来确保同一时间段内不会出现重复预约。
>
> **Scalability**
>
> > - **水平扩展**：通过增加更多的服务器节点和数据库实例，实现系统的水平扩展。
> > - **负载均衡**：使用负载均衡器将流量分配到多个服务器，提高系统的处理能力和可靠性。
> > - **数据库分片**：将数据库按照一定规则进行分片，提高数据库的读写性能和扩展能力。
> > - **缓存机制**：使用缓存（如Redis或Memcached）存储常用数据，减少数据库查询压力，提高系统响应速度。
> > - **异步处理**：对于不需要立即处理的任务（如发送通知），使用消息队列（如RabbitMQ或Kafka）进行异步处理，提高系统的处理效率。
> > - **CDN加速**：使用内容分发网络（CDN）加速静态资源的访问，提高用户的访问速度和体验。



### 10. 设计一个信用卡联名program，考虑数据安全和高可用性

> **Functional requirement: **
>
> > - 用户可以申请信用卡。
> > - 用户可以查看信用卡的消费记录和积分。
> > - 用户可以通过亚马逊账户管理信用卡信息。
> > - 用户可以使用信用卡积分兑换亚马逊商品。
> > - 系统需要支持信用卡的实时授权和交易处理。
> > - 系统需要提供账单和支付功能。
>
> **Non-Function requeirment:**
>
> > - Availiblity: 
> > - Scalability: 
> > - Performance: latency, throughput
> > - Security
> > - Consistence 
>
> **Extended requirement:**
>
> > - 
>
> **Capacity Estimation and Constraints**
>
> > how many users we have? 
> > Let’s assuem we have 200M users.  let’s suppose each user will  generate 0.1 URL per day， redirected by 10 URLs/day
> >
> > - Write QPS
> >   - Average: 200,000,000* 0.1 / (24*3600) = 231.4 QPS , Let’s say 300 QPS
> >   - Peak: 300*2 = 600 QPS 
> > - Read QPS (100 times of write)
> >   - Average: 30K QPS
> >   - Peak: 60K QPS
> > - Storage(WRITE)
> >   - 500 byte for every url, 200,000,000* 0.1* 0.5 KB= 10,000,000 KB = 10000 MB = 10 GB / DAY
> >   - 10 TB distk can use 3 years, if we store 5 years, we need 20TB including backup
> > - Memory(READ)
> >   - Let’s follow 8/2 rule, means 20% of URLs taking 80% traffic, and we can cache 20% URLs per day
> >   - Every day, we need 200M * 0.5 KB* 10 *0.2 = 200,000,000 KB = 200GB /DAY
>
> **Service:**
>
> > - **User service用户管理服务**：处理用户注册、登录、身份验证和权限管理。
> >
> > - **Card management service信用卡管理服务**：处理信用卡申请、授权、交易、积分管理等。
> > - **Bill and pay service账单和支付服务**：生成账单、处理支付和结算。
> > - **Notification service通知服务**：发送交易提醒、账单通知和积分更新。
> > - **Security service安全服务**：提供数据加密、身份验证和授权。
> > - **Monitor and log service监控和日志服务**：监控系统性能，记录操作日志和异常处理。
> >
> > **How do we detect and prevent abuse?** 
> > For instance, any service can put us out of business by consuming all our keys in the current design. To prevent abuse, we can limit users through their APIKey, how many URL they can create or access in a certain time.
>
> **Database:**
>
> > - **关系型数据库**：用于存储用户信息、信用卡信息、交易记录和账单。
> >
> >   > 1. user talbe
> >   >
> >   >    > - user_id BIGINT PRIMARY KEY AUTO_INCREMENT,    
> >   >    >
> >   >    > - username VARCHAR(255) NOT NULL,    
> >   >    >
> >   >    > - password_hash VARCHAR(255) NOT NULL,    
> >   >    >
> >   >    > - email VARCHAR(255) NOT NULL UNIQUE,    
> >   >    >
> >   >    > - phone_number VARCHAR(20),    
> >   >    >
> >   >    > - created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
> >   >
> >   > 2. CreditCard table
> >   >
> >   >    > - card_id BIGINT PRIMARY KEY AUTO_INCREMENT,    
> >   >    > - user_id BIGINT NOT NULL,    
> >   >    > - card_number VARCHAR(20) NOT NULL UNIQUE,    
> >   >    > - card_type VARCHAR(50),    
> >   >    > - expiration_date DATE,    
> >   >    > - cvv VARCHAR(4),    
> >   >    > - credit_limit DECIMAL(10, 2),    
> >   >    > - available_credit DECIMAL(10, 2),    
> >   >    > - FOREIGN KEY (user_id) REFERENCES Users(user_id)
> >
> > - **NoSQL数据库**：用于存储实时交易数据和积分信息，以提高读取性能。
> >
> >   > 1. Transaction table
> >   >
> >   >    > - transaction_id BIGINT PRIMARY KEY AUTO_INCREMENT,    
> >   >    > - card_id BIGINT NOT NULL,    
> >   >    > - transaction_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,    
> >   >    > - amount DECIMAL(10, 2),    
> >   >    > - merchant VARCHAR(255),    
> >   >    > - status VARCHAR(50),    
> >   >    > - FOREIGN KEY (card_id) REFERENCES CreditCards(card_id)
> >   >
> >   > 2. Rewards table
> >   >
> >   >    > - reward_id BIGINT PRIMARY KEY AUTO_INCREMENT,    
> >   >    > - user_id BIGINT NOT NULL,    
> >   >    > - points INT,   
> >   >    > - last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,    
> >   >    > - FOREIGN KEY (user_id) REFERENCES Users(user_id)
> >   >
> >   > 3. Bill table
> >   >
> >   >    > - bill_id BIGINT PRIMARY KEY AUTO_INCREMENT,    
> >   >    > - user_id BIGINT NOT NULL,    
> >   >    > - amount_due DECIMAL(10, 2),    
> >   >    > - due_date DATE,    
> >   >    > - status VARCHAR(50),    
> >   >    > - generated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,    
> >   >    > - FOREIGN KEY (user_id) REFERENCES Users(user_id)
>
> **Component Detail Design:**
>
> > 1. 如何确保用户数据的安全性？
> >
> >    > 我们可以通过使用SSL/TLS加密数据传输，采用强密码策略和双因素认证来保护用户账户，使用加密技术存储敏感信息（如信用卡号），并定期进行安全审计和漏洞扫描。
> >
> > 2. 如何处理系统的高可用性？
> >
> >    > 我们可以通过使用负载均衡器，将流量分配到多个服务器节点，采用数据库复制和分片技术，使用容灾备份和自动故障切换机制，确保系统的高可用性。
> >
> > 3. 如何设计积分兑换功能？
> >
> >    > 积分兑换功能可以通过一个兑换请求表来实现，当用户发起兑换请求时，将请求记录插入表中，后台服务处理兑换请求，检查用户积分余额并执行相应的商品兑换操作，更新用户积分和订单状态。
>
> **Scalability**
>
> > - **水平扩展**：通过增加更多的服务器节点和数据库实例，实现系统的水平扩展。
> > - **负载均衡**：使用负载均衡器将流量分配到多个服务器，提高系统的处理能力和可靠性。
> > - **数据库分片**：将数据库按照一定规则进行分片，提高数据库的读写性能和扩展能力。
> > - **缓存机制**：使用缓存（如Redis或Memcached）存储常用数据，减少数据库查询压力，提高系统响应速度。
> > - **异步处理**：对于不需要立即处理的任务（如发送通知、积分计算等），使用消息队列（如RabbitMQ或Kafka）进行异步处理，提高系统的处理效率。
> > - **CDN加速**：使用内容分发网络（CDN）加速静态资源的访问，提高用户的访问速度和体验。



### 11. system design, 假设公司有很多可以监测天气的sensor分散在各地，设计一个app可以让用户查看天气

### 12. Design what’s app

### 13.  Warehouse

>    > 1M product → 1000 warehouse → customer, we want to know how many of x product in y warehouse
>     >
>     > 这个题目比较讨厌, 是个高写入低读取的系统, write: 5000/sec, read: 100/sec.
>     > db里面我是这样设计的, 然后把当前的数存算好存在缓存里面, 不过没有解决高写入低读取的问题
>     > {
>     > id,
>     > productId,
>     > warehouseId,
>     > direction(in/out),
>     > productCount,
>     > timestamp
>     > }

### 14. 设计亚马逊音乐 后台数据网站

### 15. 系统设计，优惠券

### 16. web crawler

### 17. 考的是设计一个评分系统，用户可以给视频打1到5分，然后还需要计算出每个视频的平均分数，展示出来。然后还讨论了如何搜索评分，如搜索评分为4分之上的视频。

### 18. SD类似 Twitter Timeline 但所有人都贴到同个布告栏没有follower，从10个人用到10M人用怎么scale

### 19. tictactoe的game

### 20. 设计一个系统显示一天内销售最高的10个商品 ‍‍‌‌‌‍‌‍‌‌‍‍‌‌‍‍‍‍‌

### 21. 系统设计大概是在一个商业合作系统中，每天和每月获得其中一方的劳动报酬数据。用户量限定在一个范围内，不是百万级，所以之前准备的一些scale也没用上。面试官比较喜欢聊数据流，API通信之类的。

### 22. suggestion system for shopping good, when user select one item, suggest other items user might also consider to bought

### 23. online chat system

### 24. SD 设计一个least recent play song list

> user会一直在点歌, 听歌, 要实时的返回最近听过的20首歌

### 25. 需要考虑商家上传酒店信息和用户订房间两方面）

### 26. 设计一个返现系统  类似每次成交一单 有10 % 返回给 content creator

### 27. parking garage

### 28. 设计储存和搜索log的系统

### 29.     Design a loan website, find loans for users based on their preferences, once user click on the loan link, they will be redirected to the external url

### 30. SD题目是weather monitor system of washington state. 测温度, 地图, 用户端显示.

### 31. 考系统设计，设计短时间的投票系统

### 32. Design Youtube

> **Functional requirement: **
>
> > - Upload video
> > - Search video
> > - Watch video
> > - Share video
> > - Generate snapshot
> > - Comment video
> > - Like/Unlike video
>
> **Non-Function requeirment:**
>
> > - Availiblity: 
> > - Scalability: 
> > - Performance: latency, throughput
> > - Security
> > - Retime suggestion system
>
> **Extended requirement:**
>
> > - 
>
> **Capacity Estimation and Constraints**
>
> > how many users we have? 
> > 我们有150M的日活跃用户(DAU)，如果用户平均每天观看30个视频，则每秒视频观看数为:
> >
> > - Write QPS **根据经验值上传视频数总是小于观看视频数，**因此假设每上传一个视频，我们就会观看500部视频
> >
> >   - Average: 52083 / 500 = 104 视频/秒
> >   - Peak: 208 
> >
> > - Read QPS
> >
> >   - Average: 150,000,000* 30 / (24*3600) = 52083 视频/秒，**即每天视频观看数为45亿**
> >   - Peak: 100K QPS
> >
> > - Storage(WRITE) 
> >   假设在平均的情况下，一分钟视频需要50MB的存储空间(视频需要以多种格式存 储)，一秒钟内上传的视频所需的总存储量为
> >
> >   - 520 * 50 = 26000 MB/s = 26 GB/s
> >   - 10 TB distk can use 3 years, if we store 5 years, we need 20TB including backup
> >
> > - Bandwith带宽
> >
> >   - 每秒上传520分钟的视频，并假设每次上传的视频占用的带宽为 166KB/s
> >
> >     520 * 60 * 166 = 5179200 KB/s = 5 G/s
>
> **Service:**
>
> > - User Service
> >
> > - Encode Service
> >
> > - Thumb Service
> >
> > - Video Service
> >
> >   - Upload video
> >
> >     > `uploadVideo(api_dev_key, video_title, vide_description, tags[], category_id, default_language, recording_details, video_contents)`
> >     >
> >     > A successful upload will return HTTP 202 (request accepted), and once the video encoding is completed, the
> >     >
> >     > user is notified through email with a link to access the video.
> >
> >   - Search video
> >
> >     > `searchVideo(api_dev_key, search_query, user_location, maximum_videos_to_return, page_token)`
> >     >
> >     > A JSON containing information about the list of video resources matching the search query. Each video
> >     >
> >     > resource will have a video title, a thumbnail, a video creation date and how many views it has.
> >
> > **How do we detect and prevent abuse?** 
> > For instance, any service can put us out of business by consuming all our keys in the current design. To prevent abuse, we can limit users through their APIKey, how many URL they can create or access in a certain time.
>
> **Stroage:**
>
> > - **Video metadata storage - MySql**
> >
> >   > 1. **Table: video**
> >   >
> >   >    > ![image-20240519185714461](系统设计案例.assets/image-20240519185714461.png)
> >   >
> >   > 2. Table: comment
> >   >
> >   >    > - CommentID
> >   >    > -  VideoID
> >   >    > -  UserID
> >   >    > -  Comment
> >   >    > - TimeOfCreation
> >   >
> >   > 3. Table: thumbnail
> >   >
> >   >    > <img src="系统设计案例.assets/image-20240519185509562.png" alt="image-20240519185509562"  />
> >   >
> >   > 4. **Chunk Table**
> >   >
> >   >    > ![image-20240519185557762](系统设计案例.assets/image-20240519185557762.png)
> >
> > - **User metadata storage - MySql**
> >
> >   > 1. **User table**
> >   >
> >   >    > UserID, Name, email, address, age, registration details etc.
> >   >
> >   > 2. User video table
> >   >
> >   >    > ![image-20240519185831283](系统设计案例.assets/image-20240519185831283.png)
> >
> > - Thumbnail Storage
> >
> >   > -  seaweedFS 
> >   > - FastFS
> >   > - S3
> >   > - Bigtable
> >
> > - Video Storage
> >
> >   > - HDFS
>
> **Component Detail Design:**
>
> > - Video Service
> >
> >   > <img src="系统设计案例.assets/image-20240519191122303.png" alt="image-20240519191122303" style="zoom:80%;" />
> >   >
> >   > 1. 在同一个请求中，要上传大量的数据，导致整个过程会比较漫长，且失败后需要重头开始上传?
> >   >
> >   >    > 1. 视频切分。在编码方式上传中，在前端我们 只要先获取文件的二进制内容， 然后对其内容进行拆分，最后将 每个切片上传到服务端即可。
> >   >    > 2. 断点续传。视频切分后，在客户端根据哈希算法生成所 有视频切片的 chunk_id， 客户端发起上传请求，将所有 chunk_id 带 到服务端， 服务端生成 video_id & 保存目录并返回给 客户端， 客户端开始上传 chunk
> >   >    > 3. 服务端生成 video_id & 保存目录并返回给 客户端
> >   >
> >   > 2. 视频上传的过程中，如果用户主动停止上传，那么已经上传的部分如何处理?
> >   >
> >   >    > 会存一段时间，过期后删除上传部分视频
> >   >
> >   > 3. **Where would videos be stored?**
> >   >
> >   >    >  Videos can be stored in a distributed file storage system like HDFS or GlusterFS.
> >   >
> >   > 4. **How should we efficiently manage read traffic?**
> >   >
> >   >    > We should segregate our read traffic from write. Since we will be having multiple copies of each video, we can distribute our read traffic on different servers. For metadata, we can have master-slave configurations, where writes will go to master first and then replayed at all the slaves. Such configurations can cause some staleness in data, e.g. when a new video is added, its metadata would be inserted in the master first, and before it gets replayed at the slave, our slaves would not be able to see it and therefore will be returning stale results to the user. This staleness might be acceptable inour system, as it would be very short lived and the user will be able to see the new videos after a few milliseconds.
> >   >
> >   > 5. 当用户上传重复的视频时会发生什么?
> >   >
> >   >    > 两个人都上传到后端，先上传的存入后上传的删除，都在前端显示上传成功
> >   >
> >   > 6. 如何提高观看视频的流畅度?
> >   >
> >   >    > 1. 在前端加载视频列表的时候，去文件服务上读取
> >   >    > 2. 用户将鼠标移动到进度条上时，加载缩略图到本地缓存。
> >   >    > 3. 用户点开某个视频的时候，将该视频的所有缩略图加载到本地缓存。
> >
> > - Encode service
> >
> >   > 1. Why  encode?
> >   >
> >   >    > 用户可能上传各种格式的视频，需要转换成 Youtube 兼 容的格式在网页端进行播放。
> >   >
> >   > 2. 视频转码和缩略图生成如何实现?
> >   >
> >   >    > 最好还是放在一台 Server 上，因为视频在系统,内部的传输也要占用带宽
> >   >    >
> >   >    > <img src="系统设计案例.assets/image-20240519184036196.png" alt="image-20240519184036196" style="zoom:50%;" />
>
> **Scalability**
>
> > ![image-20240519191458850](系统设计案例.assets/image-20240519191458850.png)
> >
> > 1. Sharding
> >
> >    > 1. **Sharding based on UserID:** 
> >    > 2. **Sharding based on VideoID:**
> >
> > 2. Load balance
> >
> > 3. CDN
> >
> > 4. Cache



























 

























---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

> **Functional requirement: **
>
> > - 
>
> **Non-Function requeirment:**
>
> > - Availiblity: 
> > - Scalability: 
> > - Performance: latency, throughput
> > - Security
>
> **Extended requirement:**
>
> > - 
>
> **Capacity Estimation and Constraints**
>
> > how many users we have? 
> > Let’s assuem we have 200M users.  let’s suppose each user will  generate 0.1 URL per day， redirected by 10 URLs/day
> >
> > - Write QPS
> >   - Average: 200,000,000* 0.1 / (24*3600) = 231.4 QPS , Let’s say 300 QPS
> >   - Peak: 300*2 = 600 QPS 
> > - Read QPS (100 times of write)
> >   - Average: 30K QPS
> >   - Peak: 60K QPS
> > - Storage(WRITE)
> >   - 500 byte for every url, 200,000,000* 0.1* 0.5 KB= 10,000,000 KB = 10000 MB = 10 GB / DAY
> >   - 10 TB distk can use 3 years, if we store 5 years, we need 20TB including backup
> > - Memory(READ)
> >   - Let’s follow 8/2 rule, means 20% of URLs taking 80% traffic, and we can cache 20% URLs per day
> >   - Every day, we need 200M * 0.5 KB* 10 *0.2 = 200,000,000 KB = 200GB /DAY
>
> **Service:**
>
> > - Functions
> >
> > - RESTful API
> >
> > **How do we detect and prevent abuse?** 
> > For instance, any service can put us out of business by consuming all our keys in the current design. To prevent abuse, we can limit users through their APIKey, how many URL they can create or access in a certain time.
>
> **Database:**
>
> > 
>
> **Component Detail Design:**
>
> > 
>
> **Scalability**
>
> > 
>

